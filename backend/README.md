## Stress Window Ingestion & Labeling

Script: `ingest_label_windows.py`

Purpose: Flatten exported window JSON files from `ml_training_data/` and assign labels (calm / stressed) using a segment protocol plus optional self-report overrides.

### Inputs

1. Exported window files: `backend/ml_training_data/window_XXXXXX_ml_features.json` (auto-generated by `agent.py`)
2. Segments definition file (CSV or JSON lines) with columns:
   - `subject_id`
   - `session_id`
   - `segment_start` (epoch seconds)
   - `segment_end` (epoch seconds)
   - `segment_label` (e.g. `calm` or `stressed`)
   - optional: `phase`
3. (Optional) Self reports CSV: `subject_id,session_id,timestamp,self_report` (Likert 1–5)

Self report mapping: 1–2 -> calm, 4–5 -> stressed, 3 ignored.

If a self-report within ±60s of the window midpoint contradicts the segment label, it overrides with reduced confidence.

### Output

A Parquet (default) or CSV dataset with one row per window including:

- Flattened feature columns (nested keys joined with dots)
- `segment_label`, `original_segment_label`, `label`, `label_source`, `label_confidence`
- `window_mid_timestamp`, `subject_id`, `session_id`

### Usage Examples

```
python backend/ingest_label_windows.py \
	--segments-file data/segments.csv \
	--windows-dir backend/ml_training_data \
	--out-file data/dataset.parquet

python backend/ingest_label_windows.py \
	--segments-file data/segments.csv \
	--self-reports-file data/self_reports.csv \
	--windows-dir backend/ml_training_data \
	--out-file data/dataset.parquet
```

Force subject/session IDs (if not in files):

```
python backend/ingest_label_windows.py \
	--segments-file data/segments.csv \
	--subject-id S01 --session-id SES01 \
	--out-file data/dataset.parquet
```

### Sample Segments CSV

```
subject_id,session_id,segment_start,segment_end,segment_label,phase
S01,SES01,1755805200,1755805320,calm,baseline
S01,SES01,1755805320,1755805500,stressed,task
S01,SES01,1755805500,1755805620,calm,recovery
```

### Notes

- Confidence: 1.0 segment only, 0.9 segment without nearby self-report (when reports exist), 0.8 self-report only, 0.7 overridden segment.
- NaN / inf numeric values are median-imputed per column.
- Extend logic later to multi-class or weighting by `label_confidence` during model training.

## Model Training

Script: `train_stress_model.py`

Purpose: Train baseline RandomForest (calm vs stressed) using labeled dataset.

### Train

```
python backend/train_stress_model.py --data-file data/dataset.parquet --out-dir backend/models --min-confidence 0.7
```

Outputs:

- `backend/models/stress_rf.joblib`
- `backend/models/model_metadata.json`

Metadata contains feature list, medians for imputation, and cross-validated metrics (AUC, balanced accuracy, F1).

### Inference Outline

1. Flatten new window dict (same approach as ingest).
2. Order features per metadata `features` list.
3. Replace missing with `medians`.
4. `model.predict_proba([vector])[0,1]` -> stress probability.

### Next Enhancements

- Add probability smoothing over last N windows.
- Add calibration (Platt/Isotonic) if needed.
- Integrate sample weights = `label_confidence`.
- Log feature importances for explainability.

## End-to-End Quick Start

Goal: Collect windows with `agent.py`, label via protocol segments, train a model.

1. Start recording: note epoch start time (run `date +%s` right when session begins).
2. Run the agent (produces window JSON files in `backend/ml_training_data/`).
3. Generate segments file matching your protocol:

```
python backend/generate_segments.py \
	--subject S01 --session SES01 \
	--start-time <epoch_start> \
	--phases baseline:calm:120 task:stressed:180 recovery:calm:120 \
	--out data/segments.csv
```

4. Ingest & label windows:

```
python backend/ingest_label_windows.py \
	--segments-file data/segments.csv \
	--windows-dir backend/ml_training_data \
	--out-file data/dataset.parquet
```

5. Train model:

```
python backend/train_stress_model.py --data-file data/dataset.parquet --out-dir backend/models
```

6. (Later) Add self-reports and re-run ingestion for refined labels.

Now you have: labeled dataset + saved model + feature metadata. Integrate inference next.
